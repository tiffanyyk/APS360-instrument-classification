{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"code.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"DYPVwTLJygZN","colab_type":"text"},"source":["# Stage 1: Musical Instrument Classifier \n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"tqinAAcD0egh","colab_type":"text"},"source":["###Installing / Importing Packages, Mounting Google Drive"]},{"cell_type":"code","metadata":{"id":"We1JU7kQ0b7f","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":176},"executionInfo":{"status":"ok","timestamp":1596906535311,"user_tz":240,"elapsed":4849,"user":{"displayName":"Tianxu An","photoUrl":"","userId":"04744487621159126983"}},"outputId":"824ebb60-2be3-48b3-dfb8-8bc5c092b2b1"},"source":["!pip install torchaudio==0.6.0\n","!pip install torchvision==0.7.0"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torchaudio==0.6.0 in /usr/local/lib/python3.6/dist-packages (0.6.0)\n","Requirement already satisfied: torch==1.6.0 in /usr/local/lib/python3.6/dist-packages (from torchaudio==0.6.0) (1.6.0+cu101)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.6.0->torchaudio==0.6.0) (0.16.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.6.0->torchaudio==0.6.0) (1.18.5)\n","Requirement already satisfied: torchvision==0.7.0 in /usr/local/lib/python3.6/dist-packages (0.7.0+cu101)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==0.7.0) (1.18.5)\n","Requirement already satisfied: torch==1.6.0 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.7.0) (1.6.0+cu101)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.7.0) (7.0.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.6.0->torchvision==0.7.0) (0.16.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"P9ojk5rLyfj_","colab_type":"code","colab":{}},"source":["# import all libraries we need here before starting \n","\n","import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.nn.functional as F\n","import torch.optim as optim \n","import matplotlib.pyplot as plt\n","import torch.utils.data as Data\n","import torch.nn as nn\n","import torchaudio\n","import numpy as np\n","import math\n","import os \n","import shutil\n","use_cuda = True"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YmDAesWr0pWU","colab_type":"text"},"source":["###Data Processing"]},{"cell_type":"code","metadata":{"id":"tqHiJvseIXqn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":125},"executionInfo":{"status":"ok","timestamp":1596905988421,"user_tz":240,"elapsed":46636,"user":{"displayName":"Tianxu An","photoUrl":"","userId":"04744487621159126983"}},"outputId":"6e308937-4f4a-4423-9c8f-48c9f0256558"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OaQ06wxI7xre","colab_type":"code","colab":{}},"source":["%%capture\n","!unzip '/content/drive/My Drive/APS 360 Project/IRMAS-Training-Small.zip' -d '/root/'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ee56C1YvULQD","colab_type":"code","colab":{}},"source":["# clean folders\n","\n","# classes = ['cel', 'cla', 'flu', 'gac', 'gel', 'org', 'pia', 'sax', 'tru', 'vio', 'voi']\n","classes = ['gac','pia','tru','vio']\n","train_dir = \"/root/IRMAS-Small\"\n","for file in os.listdir(train_dir):\n","  if file not in classes:\n","    os.remove(os.path.join(train_dir, file))\n","    print(\"Removed {}\".format(file))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_9aheEPI07AR","colab_type":"code","colab":{}},"source":["Normalized_Max = 0.5\n","\n","def normalize_waveform(waveform, norm_max=0.5):\n","  max_magnitudes = waveform.abs().max(dim=1, keepdim=True)[0]\n","  normalized_waveforms = waveform.float().div(max_magnitudes) * norm_max\n","  return normalized_waveforms\n","\n","def audio_loader(file_path):\n","  waveform, _ = torchaudio.load(file_path)\n","  return normalize_waveform(waveform, Normalized_Max)\n","\n","audioFolder = torchvision.datasets.DatasetFolder(\"/root/IRMAS-Small\", loader=audio_loader, extensions='wav')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"01ZH_ENJ0fon","colab_type":"code","colab":{}},"source":["VALIDATION_PERCENTAGE = 0.2\n","TEST_PERCENTAGE = 0.2\n","\n","def get_data_indices(data_size):\n","  # Randomly split data into training, validation and test sets.\n","\n","  # Create a list of randomized indices of image data\n","  np.random.seed(1)\n","  indices = np.arange(data_size)\n","  np.random.shuffle(indices)\n","\n","  # Set size for each dataset\n","  validation_size = math.floor(data_size * VALIDATION_PERCENTAGE)\n","  test_size = math.floor(data_size * TEST_PERCENTAGE)\n","  training_size = data_size - validation_size - test_size\n","\n","  training_indices = indices[:training_size]\n","  val_indices = indices[training_size : training_size + validation_size]\n","  test_indices = indices[training_size + validation_size:]\n","\n","  return training_indices, val_indices, test_indices\n","\n","def get_data_loaders(folder, batch_size=64): \n","  # Load training, validation and test data.\n","  \n","  data_size = len(folder)\n","\n","  # Get training, validation and test data indices\n","  training_indices, val_indices, test_indices = get_data_indices(data_size)\n","\n","  # Create subsets\n","  training_set = torch.utils.data.Subset(folder, training_indices)\n","  val_set = torch.utils.data.Subset(folder, val_indices)\n","  test_set = torch.utils.data.Subset(folder, test_indices)\n","\n","  # Create dataloaders for each dataset\n","  train_loader = Data.DataLoader(training_set, batch_size=batch_size)\n","  validation_loader = Data.DataLoader(val_set, batch_size=batch_size)\n","  test_loader = Data.DataLoader(test_set, batch_size=batch_size)\n","\n","  return train_loader, validation_loader, test_loader"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QtRNlpU08om-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":67},"executionInfo":{"status":"ok","timestamp":1595366752494,"user_tz":240,"elapsed":205976,"user":{"displayName":"Tiffany Yau","photoUrl":"","userId":"08997438292864531844"}},"outputId":"4b867fbd-9f28-4923-ad35-4c186290d9da"},"source":["train_loader, val_loader, test_loader = get_data_loaders(audioFolder, 1)\n","\n","# Output the size of each dataset.\n","print(\"# of training examples: \", len(train_loader))\n","print(\"# of validation examples: \", len(val_loader))\n","print(\"# of test examples: \", len(test_loader))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["# of training examples:  1732\n","# of validation examples:  576\n","# of test examples:  576\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6tU7Vxad8qw1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1595366756742,"user_tz":240,"elapsed":210210,"user":{"displayName":"Tiffany Yau","photoUrl":"","userId":"08997438292864531844"}},"outputId":"32fc56b0-1c13-4013-84ec-6a8b09b2d888"},"source":["for i, data in enumerate(train_loader, 0):\n","  inputs, labels = data\n","  print(inputs[0].shape)\n","  \n","  break"],"execution_count":null,"outputs":[{"output_type":"stream","text":["torch.Size([2, 132299])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lib37w0E93dg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1595361813323,"user_tz":240,"elapsed":521,"user":{"displayName":"Ahmed Nissal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9VwLQFEJ_MtOfpfXUsU01xgifV4R3Dn381ewo6g=s64","userId":"01795038140591039031"}},"outputId":"32b037a3-a7fa-423d-f79f-89da1e7a75bc"},"source":["print(labels.item())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DqxOVn7kOxbG","colab_type":"text"},"source":["###Training the Baseline Model of Random Forest Models"]},{"cell_type":"code","metadata":{"id":"O-BFKnVDUf0n","colab_type":"code","colab":{}},"source":["train_inputs = []\n","train_labels = []\n","for i,data in enumerate(train_loader,0):\n","    inputs,labels = data\n","    train_inputs.append(torch.reshape(inputs[0],(-1,)).numpy())\n","    train_labels.append(labels.item())\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qz-avP3efJK_","colab_type":"code","colab":{}},"source":["train_inputs = np.array(train_inputs)\n","train_labels = np.array(train_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NDBKeRFsg3Ub","colab_type":"code","colab":{}},"source":["val_inputs = []\n","val_labels = []\n","for i,data in enumerate(val_loader,0):\n","    inputs,labels = data\n","    val_inputs.append(torch.reshape(inputs[0],(-1,)).numpy())\n","    val_labels.append(labels.item())\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xFrUJ15thVLN","colab_type":"code","colab":{}},"source":["val_inputs = np.array(val_inputs)\n","val_labels = np.array(val_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"czklke3esn7W","colab_type":"code","colab":{}},"source":["#baseline model training here\n","# Random Forest\n","from sklearn.ensemble import RandomForestClassifier\n","model = RandomForestClassifier(n_estimators=500)\n","\n","# Fit the model to our training data\n","model.fit(train_inputs, train_labels)\n","\n","# Make predictions\n","val_predicted = model.predict(val_inputs)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GapN5zuukhQr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593963991012,"user_tz":-480,"elapsed":1114,"user":{"displayName":"Reynold Chan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjT60WS6_obYMUr1XcwATSo-6wIsB7qDf8TJoApUg=s64","userId":"13625816981707293531"}},"outputId":"2c89dce1-3356-4487-e5cc-ab02f84006d0"},"source":["correct = 0\n","for i in range(len(val_predicted)):\n","    if val_predicted[i] == val_labels[i]:\n","        correct +=1 \n","print(\"accuracy of baseline model: {0}\".format(correct/len(val_predicted)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["accuracy of baseline model: 0.1849366144668158\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zynCdvmm041j","colab_type":"text"},"source":["###Convolutional Network Architecture"]},{"cell_type":"code","metadata":{"id":"K-tc2Obuhy-8","colab_type":"code","colab":{}},"source":["# we could start off with a simple CNN architecture... to improve use similar architecture as AlexNet or others for CNN approach...\n","# input size is huge 132,299: we need to find a way to downscale while extracting relevant features of the input.\n","\n","num_classes = len(classes)\n","\n","class MyNet(nn.Module):\n","    def __init__(self):\n","        super(MyNet, self).__init__()\n","        self.name = \"net\"\n","        self.conv1 = nn.Conv1d(2, 20, 11) # input channel is 2 for audio files\n","        self.pool = nn.MaxPool2d(2, 2) \n","        self.conv2 = nn.Conv1d(10, 5, 9)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.conv3 = nn.Conv1d(2, 4, 5)\n","        self.pool2 = nn.MaxPool2d(4, 4)\n","        self.fc1 = nn.Linear(8266, 500)\n","        self.fc2 = nn.Linear(500, num_classes)\n","\n","\n","    def forward(self, x):\n","        x = self.pool(F.relu(self.conv1(x))) # x_input = 132,299 -> conv1: (132,299-11+1)/1 132289 -> maxpool2d: x_output = 66,144\n","        x = self.pool(F.relu(self.conv2(x))) # x_input = 66,144 -> conv2: 66,144-9+1/1 -> 66,136 -> maxpool2d: x_output = 33,068\n","        x = self.pool2(F.relu(self.conv3(x))) # x_input = 33,068 -> conv3: 33,068-5+1/1 -> 33,064 -> maxpool2d: x_output = 33,068/4 -> 8266\n","        x = x.view(-1, 8266)\n","        x = F.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        return x\n","print('Convolutional Neural Network Architecture Selected')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5pi2GgdHWnUk","colab_type":"code","colab":{}},"source":["class MyNet2(nn.Module):\n","    def __init__(self):\n","        super(MyNet2, self).__init__()\n","        self.name = \"net\"\n","        self.conv1 = nn.Conv1d(2, 20, 11) # input channel is 2 for audio files\n","        self.pool = nn.MaxPool2d(2, 2) \n","        self.conv2 = nn.Conv1d(10, 5, 9)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.conv3 = nn.Conv1d(2, 4, 5)\n","        self.pool2 = nn.MaxPool2d(4, 4)\n","        self.fc1 = nn.Linear(8266, 100)\n","        self.fc2 = nn.Linear(100, num_classes)\n","\n","\n","    def forward(self, x):\n","        x = self.pool(F.relu(self.conv1(x))) # x_input = 132,299 -> conv1: (132,299-11+1)/1 132289 -> maxpool2d: x_output = 66,144\n","        x = self.pool(F.relu(self.conv2(x))) # x_input = 66,144 -> conv2: 66,144-9+1/1 -> 66,136 -> maxpool2d: x_output = 33,068\n","        x = self.pool2(F.relu(self.conv3(x))) # x_input = 33,068 -> conv3: 33,068-5+1/1 -> 33,064 -> maxpool2d: x_output = 33,068/4 -> 8266\n","        x = x.view(-1, 8266)\n","        x = F.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RIVX9qQIGTGt","colab_type":"code","colab":{}},"source":["class MyNet3(nn.Module):\n","    def __init__(self):\n","        super(MyNet3, self).__init__()\n","        self.name = \"net\"\n","        self.conv1 = nn.Conv1d(2, 20, 11) # input channel is 2 for audio files\n","        self.pool = nn.MaxPool2d(2, 2) \n","        self.conv2 = nn.Conv1d(10, 5, 9)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.conv3 = nn.Conv1d(2, 4, 5)\n","        self.pool2 = nn.MaxPool2d(4, 4)\n","        self.fc1 = nn.Linear(8266, num_classes)\n","\n","\n","    def forward(self, x):\n","        x = self.pool(F.relu(self.conv1(x))) # x_input = 132,299 -> conv1: (132,299-11+1)/1 132289 -> maxpool2d: x_output = 66,144\n","        x = self.pool(F.relu(self.conv2(x))) # x_input = 66,144 -> conv2: 66,144-9+1/1 -> 66,136 -> maxpool2d: x_output = 33,068\n","        x = self.pool2(F.relu(self.conv3(x))) # x_input = 33,068 -> conv3: 33,068-5+1/1 -> 33,064 -> maxpool2d: x_output = 33,068/4 -> 8266\n","        x = x.view(-1, 8266)\n","        x = self.fc1(x)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BVzkZI1dXZQR","colab_type":"code","colab":{}},"source":["class MyNet4(nn.Module):\n","    def __init__(self):\n","        super(MyNet4, self).__init__()\n","        self.name = \"net\"\n","        self.conv1 = nn.Conv1d(2, 20, 11) # input channel is 2 for audio files\n","        self.pool = nn.MaxPool2d(2, 2) \n","        self.conv2 = nn.Conv1d(10, 5, 9)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.conv3 = nn.Conv1d(2, 4, 5)\n","        self.pool2 = nn.MaxPool2d(4, 4)\n","        self.fc1 = nn.Linear(8266, num_classes)\n","\n","\n","    def forward(self, x):\n","        x = self.pool(F.leaky_relu(self.conv1(x))) # x_input = 132,299 -> conv1: (132,299-11+1)/1 132289 -> maxpool2d: x_output = 66,144\n","        x = self.pool(F.leaky_relu(self.conv2(x))) # x_input = 66,144 -> conv2: 66,144-9+1/1 -> 66,136 -> maxpool2d: x_output = 33,068\n","        x = self.pool2(F.leaky_relu(self.conv3(x))) # x_input = 33,068 -> conv3: 33,068-5+1/1 -> 33,064 -> maxpool2d: x_output = 33,068/4 -> 8266\n","        x = x.view(-1, 8266)\n","        x = self.fc1(x)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TTCnQmpthXSF","colab_type":"code","colab":{}},"source":["class MyNet5(nn.Module):\n","    def __init__(self):\n","        super(MyNet5, self).__init__()\n","        self.name = \"net\"\n","        self.conv1 = nn.Conv1d(2, 20, 11) # input channel is 2 for audio files\n","        self.pool = nn.MaxPool2d(2, 2) \n","        self.conv2 = nn.Conv1d(20, 5, 9)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.conv3 = nn.Conv1d(5, 1, 5)\n","        self.pool2 = nn.MaxPool2d(4, 4)\n","        self.fc1 = nn.Linear(8266, num_classes)\n","        self.bn1 = nn.BatchNorm1d(10)\n","        self.bn2 = nn.BatchNorm1d(2)\n","        self.bn3 = nn.BatchNorm1d(1)\n","\n","    def forward(self, x):\n","        x = self.bn1(self.pool(F.leaky_relu(self.conv1(x)))) # x_input = 132,299 -> conv1: (132,299-11+1)/1 132289 -> maxpool2d: x_output = 66,144\n","        x = self.bn2(self.pool(F.leaky_relu(self.conv2(x)))) # x_input = 66,144 -> conv2: 66,144-9+1/1 -> 66,136 -> maxpool2d: x_output = 33,068\n","        x = self.bn3(self.pool2(F.leaky_relu(self.conv3(x)))) # x_input = 33,068 -> conv3: 33,068-5+1/1 -> 33,064 -> maxpool2d: x_output = 33,068/4 -> 8266\n","        x = x.view(-1, 8266)\n","        x = self.fc1(x)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kh3iwmEZhlpE","colab_type":"code","colab":{}},"source":["class MyNet6(nn.Module):\n","    def __init__(self):\n","        super(MyNet6, self).__init__()\n","        self.name = \"net\"\n","        self.conv1 = nn.Conv1d(2, 20, 11) # input channel is 2 for audio files\n","        self.pool = nn.MaxPool2d(2, 2) \n","        self.conv2 = nn.Conv1d(10, 5, 9)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.conv3 = nn.Conv1d(2, 4, 5)\n","        self.pool2 = nn.MaxPool2d(4, 4)\n","        self.fc1 = nn.Linear(8266, num_classes)\n","        self.bn1 = nn.BatchNorm1d(10)\n","        self.bn2 = nn.BatchNorm1d(2)\n","        self.bn3 = nn.BatchNorm1d(1)\n","\n","    def forward(self, x):\n","        x = self.bn1(self.pool(F.relu(self.conv1(x)))) # x_input = 132,299 -> conv1: (132,299-11+1)/1 132289 -> maxpool2d: x_output = 66,144\n","        x = self.bn2(self.pool(F.relu(self.conv2(x)))) # x_input = 66,144 -> conv2: 66,144-9+1/1 -> 66,136 -> maxpool2d: x_output = 33,068\n","        x = self.bn3(self.pool2(F.relu(self.conv3(x)))) # x_input = 33,068 -> conv3: 33,068-5+1/1 -> 33,064 -> maxpool2d: x_output = 33,068/4 -> 8266\n","        x = x.view(-1, 8266)\n","        x = self.fc1(x)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0tEFccye0ox1","colab_type":"code","colab":{}},"source":["class MyNet7(nn.Module):\n","    def __init__(self):\n","        super(MyNet7, self).__init__()\n","        self.name = \"net\"\n","        self.conv1 = nn.Conv1d(2, 16, 11) # input channel is 2 for audio files\n","        self.conv2 = nn.Conv1d(16, 32, 9)\n","        self.conv3 = nn.Conv1d(32, 8, 5)\n","        self.conv4 = nn.Conv1d(8, 2, 5)\n","        self.fc1 = nn.Linear(1030, num_classes)\n","        self.pool = nn.MaxPool1d(2, 2) \n","        self.pool2 = nn.MaxPool1d(4, 4)\n","        self.bn1 = nn.BatchNorm1d(16)\n","        self.bn2 = nn.BatchNorm1d(32)\n","        self.bn3 = nn.BatchNorm1d(8)\n","        self.bn4 = nn.BatchNorm1d(2)\n","        self.drop = nn.Dropout(0.2)\n","\n","    def forward(self, x):\n","        x = self.bn1(self.pool2(F.relu(self.conv1(x)))) \n","        x = self.bn2(self.pool2(F.relu(self.conv2(x)))) \n","        x = self.bn3(self.pool2(F.relu(self.conv3(x)))) \n","        x = self.bn4(self.pool2(F.relu(self.conv4(x))))\n","        # print(np.shape(x))\n","        x = self.drop(x)\n","        x = x.view(-1, 1030)\n","        x = self.fc1(x)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BXdjJUQt_xpa","colab_type":"code","colab":{}},"source":["class MyNet8(nn.Module):\n","    def __init__(self):\n","        super(MyNet8, self).__init__()\n","        self.name = \"net\"\n","        self.conv1 = nn.Conv1d(2, 16, 11) # input channel is 2 for audio files\n","        self.conv2 = nn.Conv1d(16, 32, 9)\n","        self.conv3 = nn.Conv1d(32, 8, 5)\n","        self.conv4 = nn.Conv1d(8, 2, 5)\n","        self.fc1 = nn.Linear(1030, num_classes)\n","        self.pool = nn.MaxPool1d(2, 2) \n","        self.pool2 = nn.MaxPool1d(4, 4)\n","        self.bn1 = nn.BatchNorm1d(16)\n","        self.bn2 = nn.BatchNorm1d(32)\n","        self.bn3 = nn.BatchNorm1d(8)\n","        self.bn4 = nn.BatchNorm1d(2)\n","        self.drop = nn.Dropout(0.3)\n","\n","    def forward(self, x):\n","        x = self.bn1(self.pool2(F.relu(self.conv1(x)))) \n","        x = self.bn2(self.pool2(F.relu(self.conv2(x)))) \n","        x = self.bn3(self.pool2(F.relu(self.conv3(x)))) \n","        x = self.bn4(self.pool2(F.relu(self.conv4(x))))\n","        # print(np.shape(x))\n","        x = self.drop(x)\n","        x = x.view(-1, 1030)\n","        x = self.fc1(x)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vWYPZc4-0lDG","colab_type":"code","colab":{}},"source":["class MyNet9(nn.Module):\n","    def __init__(self):\n","        super(MyNet9, self).__init__()\n","        self.name = \"net\"\n","        self.conv1 = nn.Conv1d(2, 16, 11) # input channel is 2 for audio files\n","        self.conv2 = nn.Conv1d(16, 32, 9)\n","        self.conv3 = nn.Conv1d(32, 16, 7)\n","        self.conv4 = nn.Conv1d(16, 8, 5)\n","        self.conv5 = nn.Conv1d(8, 2, 5)\n","        self.fc1 = nn.Linear(1030, num_classes)\n","        self.pool = nn.MaxPool1d(2, 2) \n","        self.pool2 = nn.MaxPool1d(4, 4)\n","        self.bn1 = nn.BatchNorm1d(16)\n","        self.bn2 = nn.BatchNorm1d(32)\n","        self.bn3 = nn.BatchNorm1d(16)\n","        self.bn4 = nn.BatchNorm1d(8)\n","        self.bn5 = nn.BatchNorm1d(2)\n","        self.drop = nn.Dropout(0.3)\n","\n","    def forward(self, x):\n","        x = self.bn1(self.pool(F.relu(self.conv1(x)))) \n","        x = self.bn2(self.pool(F.relu(self.conv2(x)))) \n","        x = self.bn3(self.pool2(F.relu(self.conv3(x)))) \n","        x = self.bn4(self.pool2(F.relu(self.conv4(x))))\n","        x = self.bn5(self.pool2(F.relu(self.conv5(x))))\n","        # print(np.shape(x))\n","        x = self.drop(x)\n","        x = x.view(-1, 1030)\n","        x = self.fc1(x)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8SHSxOwL0lzt","colab_type":"code","colab":{}},"source":["class MyNet10(nn.Module):\n","    def __init__(self):\n","        super(MyNet10, self).__init__()\n","        self.name = \"net\"\n","        self.conv1 = nn.Conv1d(2, 16, 11) # input channel is 2 for audio files\n","        self.conv2 = nn.Conv1d(16, 32, 9)\n","        self.conv3 = nn.Conv1d(32, 16, 7)\n","        self.conv4 = nn.Conv1d(16, 8, 5)\n","        self.conv5 = nn.Conv1d(8, 2, 5)\n","        self.fc1 = nn.Linear(514, num_classes)\n","        self.pool = nn.MaxPool1d(2, 2) \n","        self.pool2 = nn.MaxPool1d(4, 4)\n","        self.bn1 = nn.BatchNorm1d(16)\n","        self.bn2 = nn.BatchNorm1d(32)\n","        self.bn3 = nn.BatchNorm1d(16)\n","        self.bn4 = nn.BatchNorm1d(8)\n","        self.bn5 = nn.BatchNorm1d(2)\n","        self.drop = nn.Dropout(0.3)\n","\n","    def forward(self, x):\n","        x = self.bn1(self.pool(F.relu(self.conv1(x)))) \n","        x = self.bn2(self.pool2(F.relu(self.conv2(x)))) \n","        x = self.bn3(self.pool2(F.relu(self.conv3(x)))) \n","        x = self.bn4(self.pool2(F.relu(self.conv4(x))))\n","        x = self.bn5(self.pool2(F.relu(self.conv5(x))))\n","        # print(np.shape(x))\n","        x = self.drop(x)\n","        x = x.view(-1, 514)\n","        x = self.fc1(x)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JU35PcZ00n7_","colab_type":"code","colab":{}},"source":["class MyNet11(nn.Module):\n","    def __init__(self):\n","        super(MyNet11, self).__init__()\n","        self.name = \"net\"\n","        self.conv1 = nn.Conv1d(2, 32, 11) # input channel is 2 for audio files\n","        self.conv2 = nn.Conv1d(32, 64, 9)\n","        self.conv3 = nn.Conv1d(64, 32, 7)\n","        self.conv4 = nn.Conv1d(32, 16, 5)\n","        self.conv5 = nn.Conv1d(16, 2, 5)\n","        self.fc1 = nn.Linear(514, num_classes)\n","        self.pool = nn.MaxPool1d(2, 2) \n","        self.pool2 = nn.MaxPool1d(4, 4)\n","        self.bn1 = nn.BatchNorm1d(32)\n","        self.bn2 = nn.BatchNorm1d(64)\n","        self.bn3 = nn.BatchNorm1d(32)\n","        self.bn4 = nn.BatchNorm1d(16)\n","        self.bn5 = nn.BatchNorm1d(2)\n","        self.drop = nn.Dropout(0.3)\n","\n","    def forward(self, x):\n","        x = self.bn1(self.pool(F.relu(self.conv1(x)))) \n","        x = self.bn2(self.pool2(F.relu(self.conv2(x)))) \n","        x = self.bn3(self.pool2(F.relu(self.conv3(x)))) \n","        x = self.bn4(self.pool2(F.relu(self.conv4(x))))\n","        x = self.bn5(self.pool2(F.relu(self.conv5(x))))\n","        # print(np.shape(x))\n","        x = self.drop(x)\n","        x = x.view(-1, 514)\n","        x = self.fc1(x)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2hUwF27YkrEx","colab_type":"code","colab":{}},"source":["class MyNet12(nn.Module):\n","    def __init__(self):\n","        super(MyNet12, self).__init__()\n","        self.name = \"net\"\n","        self.conv1 = nn.Conv1d(2, 64, 11) # input channel is 2 for audio files\n","        self.conv2 = nn.Conv1d(64, 64, 9)\n","        self.conv3 = nn.Conv1d(64, 64, 7)\n","        self.conv4 = nn.Conv1d(64, 32, 5)\n","        self.conv5 = nn.Conv1d(32, 2, 5)\n","        self.fc1 = nn.Linear(514, num_classes)\n","        self.pool = nn.MaxPool1d(2, 2) \n","        self.pool2 = nn.MaxPool1d(4, 4)\n","        self.bn1 = nn.BatchNorm1d(64)\n","        self.bn2 = nn.BatchNorm1d(64)\n","        self.bn3 = nn.BatchNorm1d(64)\n","        self.bn4 = nn.BatchNorm1d(32)\n","        self.bn5 = nn.BatchNorm1d(2)\n","        self.drop = nn.Dropout(0.3)\n","\n","    def forward(self, x):\n","        x = self.bn1(self.pool(F.relu(self.conv1(x)))) \n","        x = self.bn2(self.pool2(F.relu(self.conv2(x)))) \n","        x = self.bn3(self.pool2(F.relu(self.conv3(x)))) \n","        x = self.bn4(self.pool2(F.relu(self.conv4(x))))\n","        x = self.bn5(self.pool2(F.relu(self.conv5(x))))\n","        # print(np.shape(x))\n","        x = self.drop(x)\n","        x = x.view(-1, 514)\n","        x = self.fc1(x)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7uVpGodtk43a","colab_type":"code","colab":{}},"source":["class MyNet13(nn.Module):\n","    def __init__(self):\n","        super(MyNet13, self).__init__()\n","        self.name = \"net\"\n","        self.conv1 = nn.Conv1d(2, 64, 11) # input channel is 2 for audio files\n","        self.conv2 = nn.Conv1d(64, 64, 9)\n","        self.conv3 = nn.Conv1d(64, 64, 7)\n","        self.conv4 = nn.Conv1d(64, 32, 5)\n","        self.conv5 = nn.Conv1d(32, 16, 5)\n","        self.conv6 = nn.Conv1d(16, 2, 5)\n","        self.fc1 = nn.Linear(254, num_classes)\n","        self.pool = nn.MaxPool1d(2, 2) \n","        self.pool2 = nn.MaxPool1d(4, 4)\n","        self.bn1 = nn.BatchNorm1d(64)\n","        self.bn2 = nn.BatchNorm1d(64)\n","        self.bn3 = nn.BatchNorm1d(64)\n","        self.bn4 = nn.BatchNorm1d(32)\n","        self.bn5 = nn.BatchNorm1d(16)\n","        self.bn6 = nn.BatchNorm1d(2)\n","        self.drop = nn.Dropout(0.3)\n","\n","    def forward(self, x):\n","        x = self.bn1(self.pool(F.relu(self.conv1(x)))) \n","        x = self.bn2(self.pool(F.relu(self.conv2(x)))) \n","        x = self.bn3(self.pool2(F.relu(self.conv3(x)))) \n","        x = self.bn4(self.pool2(F.relu(self.conv4(x))))\n","        x = self.bn5(self.pool2(F.relu(self.conv5(x))))\n","        x = self.bn6(self.pool2(F.relu(self.conv6(x))))\n","        # print(np.shape(x))\n","        x = self.drop(x)\n","        x = x.view(-1, 254)\n","        x = self.fc1(x)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gr4ych7G1Bl3","colab_type":"text"},"source":["###Training Code"]},{"cell_type":"code","metadata":{"id":"sDrYPTU5gsrr","colab_type":"code","colab":{}},"source":["def get_accuracy(model, loader):\n","    correct = 0\n","    total = 0\n","    for inputs, labels in loader:\n","        if use_cuda and torch.cuda.is_available():\n","           inputs = inputs.cuda()\n","           labels = labels.cuda()\n","        output = model(inputs)\n","        #select index with maximum prediction score\n","        pred = output.max(1, keepdim=True)[1]\n","        correct += pred.eq(labels.view_as(pred)).sum().item()\n","        total += inputs.shape[0]\n","    return correct / total\n","\n","def train(model, train_loader=None, valid_loader=None, batch_size=64, num_epochs=5, learning_rate=1e-4, checkpoint=False, checkpoint_name=None, checkpoint_bestonly=False): \n","    torch.manual_seed(1)\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","    best_valacc = 0.0\n","\n","    if train_loader is not None and valid_loader is not None:\n","        pass\n","    else:\n","        train_loader, val_loader, _ = get_data_loaders(audioFolder, batch_size) \n","\n","    epoch_plot, losses, val_losses, train_acc, val_acc = [], [], [], [], []\n","    for epoch in range(num_epochs):\n","        total_train_loss = 0\n","        num_train_batch = 0\n","        for inputs, labels in iter(train_loader):\n","\n","            if use_cuda and torch.cuda.is_available():\n","                inputs = inputs.cuda()\n","                labels = labels.cuda()\n","\n","            \n","            pred = model(inputs)\n","            loss = criterion(pred, labels)\n","            loss.backward()\n","            optimizer.step()\n","            optimizer.zero_grad()\n","            \n","            total_train_loss = total_train_loss + (float(loss.item()) /len(inputs))\n","            num_train_batch += 1\n","        total_train_loss = total_train_loss / num_train_batch #/ 21502\n","        losses.append(float(total_train_loss))\n","        train_acc.append(get_accuracy(model,train_loader))\n","        \n","        # make validation predictions and calculate loss\n","        total_val_loss = 0\n","        num_val_batch = 0\n","        for inputs, labels in iter(val_loader):\n","            \n","            if use_cuda and torch.cuda.is_available():\n","                inputs = inputs.cuda()\n","                labels = labels.cuda()\n","\n","            pred = model(inputs)\n","            val_loss = criterion(pred,labels)\n","            \n","            total_val_loss = total_val_loss + (float(val_loss.item()) /len(inputs))\n","            num_val_batch += 1\n","        total_val_loss = total_val_loss / num_val_batch #/ 4608\n","        val_losses.append(float(total_val_loss))\n","        val_acc.append(get_accuracy(model,val_loader))\n","\n","        epoch_plot.append(epoch)\n","        print('Epoch:{}, Loss:{:.4f}, Val_Loss:{:.4f}, Train_acc:{:.4f}, Val_acc:{:.4f}'.format(\n","            epoch+1,\n","            float(total_train_loss),\n","            float(total_val_loss),\n","            float(train_acc[epoch]),\n","            float(val_acc[epoch])))\n","\n","        # Save the current model (checkpoint) to a file\n","        if checkpoint:\n","            if (checkpoint_bestonly and val_acc[-1] > best_valacc):\n","                best_valacc = val_acc[-1]\n","                if checkpoint_name is not None:\n","                    model_path = \"/content/drive/My Drive/APS 360 Project/saved_models/{}_batch_size={}_lr={}_best\".format(checkpoint_name,batch_size,learning_rate,epoch)\n","                else:\n","                    model_path = \"/content/drive/My Drive/APS 360 Project/saved_models/batch_size={}_lr={}_best\".format(batch_size,learning_rate,epoch)\n","                torch.save(model.state_dict(), model_path)\n","            elif not checkpoint_bestonly:\n","                if checkpoint_name is not None:\n","                    model_path = \"/content/drive/My Drive/APS 360 Project/saved_models/{}_batch_size={}_lr={}_epoch={}\".format(checkpoint_name,batch_size,learning_rate,epoch)\n","                else:\n","                    model_path = \"/content/drive/My Drive/APS 360 Project/saved_models/batch_size={}_lr={}_epoch={}\".format(batch_size,learning_rate,epoch)\n","                torch.save(model.state_dict(), model_path)\n","\n","    # plotting\n","    plt.title(\"Training Curve\")\n","    plt.plot(epoch_plot, losses, label=\"Train\")\n","    plt.plot(epoch_plot, val_losses, label=\"Validation\")\n","    plt.xlabel(\"Iterations\")\n","    plt.ylabel(\"Loss\")\n","    plt.legend(loc='best')\n","    plt.show()\n","\n","    plt.title(\"Training Curve\")\n","    plt.plot(epoch_plot, train_acc, label=\"Train\")\n","    plt.plot(epoch_plot, val_acc, label=\"Validation\")\n","    plt.xlabel(\"Iterations\")\n","    plt.ylabel(\"Accuracy\")\n","    plt.legend(loc='best')\n","    plt.show()\n","\n","    print(\"Final Training Accuracy: {}\".format(train_acc[-1]))\n","    print(\"Final Validation Accuracy: {}\".format(val_acc[-1]))\n","    print (\"Maximum validation accuracy for this model is:\", max(val_acc),\n","           \"at epoch\", epoch_plot[val_acc.index(max(val_acc))],\"\\n\")\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5OmjjBX6Ga1N","colab_type":"code","colab":{}},"source":["torch.cuda.empty_cache()\n","model = MyNet8()\n","if use_cuda and torch.cuda.is_available():\n","    model = model.cuda()\n","train(model, learning_rate=0.0003, num_epochs=30, batch_size=32, checkpoint=True, checkpoint_name='MyNet8', checkpoint_bestonly=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NjNE7zsIUtau","colab_type":"text"},"source":["### Check Test Accuracy"]},{"cell_type":"code","metadata":{"id":"kLZLAdCfUwi8","colab_type":"code","colab":{}},"source":["# load in best model and check training accuracy\n","stage_1_model = MyNet13()\n","saved_model = '/content/drive/My Drive/APS 360 Project/saved_models/MyNet13_batch_size=16_lr=0.0003_best_0.7257valacc'\n","stage_1_model.load_state_dict(torch.load(saved_model))\n","train_loader, val_loader, test_loader = get_data_loaders(audioFolder, 16) \n","print(get_accuracy(stage_1_model.eval().cuda(),test_loader))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0-kaUa8EjmrU","colab_type":"text"},"source":["# Stage 2: Multi-Instrument Identification"]},{"cell_type":"markdown","metadata":{"id":"RG79gat1kLdW","colab_type":"text"},"source":["###Data Processing"]},{"cell_type":"code","metadata":{"id":"EiUqvx25fQ5K","colab_type":"code","colab":{}},"source":["# Combine audio files\n","import os\n","\n","input_dir_path = \"/root/IRMAS-Small\"\n","combined_dir_path = \"/root/IRMAS-Combine\"\n","\n","# Make a directory in /root/\n","os.mkdir(combined_dir_path)\n","\n","class_directories = os.listdir(input_dir_path)\n","file_count = len(os.listdir(os.path.join(input_dir_path, class_directories[0])))\n","\n","# create files with combined and normalized audio\n","for i in range(len(class_directories)):\n","  for j in range(i+1, len(class_directories)):\n","    dir_name = class_directories[i] + '+' + class_directories[j]\n","    dir_path = os.path.join(combined_dir_path, dir_name)\n","    if os.path.isdir(dir_path):\n","      for file_name_to_remove in os.listdir(dir_path):\n","        os.remove(os.path.join(dir_path, file_name_to_remove))\n","      os.rmdir(dir_path)\n","    os.mkdir(dir_path)\n","    class1_names = os.listdir(os.path.join(input_dir_path, class_directories[i]))\n","    class2_names = os.listdir(os.path.join(input_dir_path, class_directories[j]))\n","    for file_index in range(file_count):\n","      wave1, sample_rate = torchaudio.load(os.path.join(input_dir_path, class_directories[i], class1_names[file_index]))\n","      wave2, sample_rate = torchaudio.load(os.path.join(input_dir_path, class_directories[j], class2_names[file_index]))\n","      wave1 = normalize_waveform(wave1, 0.5)\n","      wave2 = normalize_waveform(wave2, 0.5)\n","      combined_wave = wave1 + wave2\n","      combined_wave = normalize_waveform(combined_wave, 0.5)\n","      file_name = dir_name + str(file_index) + \".wav\"\n","      torchaudio.save(os.path.join(dir_path, file_name), combined_wave, sample_rate=sample_rate)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UEii87aNmRfn","colab_type":"code","colab":{}},"source":["from zipfile import ZipFile\n","\n","# Create zip file for combined audio files and folders\n","folder_paths = [os.path.join(combined_dir_path, folder_name) for folder_name in os.listdir(combined_dir_path)]\n","# writing files to a zipfile \n","with ZipFile('/root/IRMAS-Combine.zip','w') as zip: \n","  # writing each file one by one \n","  for folder in folder_paths: \n","    for file_name in os.listdir(folder):\n","      zip.write(os.path.join(folder, file_name), os.path.join(\"IRMAS-Combine\", os.path.basename(folder), file_name))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6pHVCtHthD04","colab_type":"code","colab":{}},"source":["# copy saved zip file from /root to our shared drive folder\n","!cp '/root/IRMAS-Combine.zip' '/content/drive/My Drive/APS 360 Project/'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vIRaA7tPXJke","colab_type":"code","colab":{}},"source":["# Unzip combined dataset to root directory\n","%%capture\n","!unzip '/content/drive/My Drive/APS 360 Project/IRMAS-Combine.zip' -d '/root/'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PQj4zTRkW866","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":352},"executionInfo":{"status":"error","timestamp":1596906101853,"user_tz":240,"elapsed":295,"user":{"displayName":"Tianxu An","photoUrl":"","userId":"04744487621159126983"}},"outputId":"9f1b14b2-6f9b-478f-bb7b-dd762d49b434"},"source":["one_hot_class_targets = np.array([[1, 1, 0, 0],\n","                                  [1, 0, 1, 0],\n","                                  [1, 0, 0, 1],\n","                                  [0, 1, 1, 0],\n","                                  [0, 1, 0, 1],\n","                                  [0, 0, 1, 1]])  # Order: gac, pia, tru, vio\n","\n","def get_one_hot_targets(class_index):\n","  return one_hot_class_targets[class_index]\n","\n","def audio_loader(file_path):\n","  waveform, _ = torchaudio.load(file_path)\n","  return waveform\n","\n","target_transform = transforms.Compose([\n","                                transforms.Lambda(get_one_hot_targets)\n","                               ])\n","\n","combined_audio_folder = torchvision.datasets.DatasetFolder(\"/root/IRMAS-Combine\", loader=audio_loader, target_transform=target_transform, extensions='wav')"],"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-cbf6996db2f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m                                ])\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mcombined_audio_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDatasetFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/root/IRMAS-Combine\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maudio_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'wav'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m     92\u001b[0m         super(DatasetFolder, self).__init__(root, transform=transform,\n\u001b[1;32m     93\u001b[0m                                             target_transform=target_transform)\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_find_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m_find_classes\u001b[0;34m(self, dir)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mNo\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0msubdirectory\u001b[0m \u001b[0mof\u001b[0m \u001b[0manother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \"\"\"\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mcls_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/root/IRMAS-Combine'"]}]},{"cell_type":"markdown","metadata":{"id":"W8aveFOilk76","colab_type":"text"},"source":["###Baseline Model"]},{"cell_type":"code","metadata":{"id":"hfNIHfcVGCkw","colab_type":"code","colab":{}},"source":["train_loader, val_loader, test_loader = get_data_loaders(combined_audio_folder, 1)\n","\n","# Output the size of each dataset.\n","print(\"# of training examples: \", len(train_loader))\n","print(\"# of validation examples: \", len(val_loader))\n","print(\"# of test examples: \", len(test_loader))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xAvWdqBZGGe5","colab_type":"code","colab":{}},"source":["#Since the label is in hotline encoding, we can turn that into a bit 2 repersentation of the label\n","\n","train_inputs = []\n","train_labels = []\n","for i,data in enumerate(train_loader,0):\n","    inputs,labels = data\n","    train_inputs.append(torch.reshape(inputs[0],(-1,)).numpy())\n","    res = int(\"\".join(str(x) for x in np.array(torch.reshape(labels[0],(-1,)).numpy())), 2)  \n","    train_labels.append(res)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UgjeDMjoGKGy","colab_type":"code","colab":{}},"source":["train_inputs = np.array(train_inputs)\n","train_labels = np.array(train_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HHPmF6PoGIj6","colab_type":"code","colab":{}},"source":["val_inputs = []\n","val_labels = []\n","for i,data in enumerate(val_loader,0):\n","    inputs,labels = data\n","    val_inputs.append(torch.reshape(inputs[0],(-1,)).numpy())\n","    res = int(\"\".join(str(x) for x in np.array(torch.reshape(labels[0],(-1,)).numpy())), 2)  \n","    val_labels.append(res)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nagefSkKGQDI","colab_type":"code","colab":{}},"source":["val_inputs = np.array(val_inputs)\n","val_labels = np.array(val_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4ZIYoq05GSdu","colab_type":"code","colab":{}},"source":["#baseline model training here\n","# Random Forest\n","from sklearn.ensemble import RandomForestClassifier\n","model = RandomForestClassifier(n_estimators=100)\n","\n","# Fit the model to our training data\n","model.fit(train_inputs, train_labels)\n","\n","# Make predictions\n","val_predicted = model.predict(val_inputs)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t7RCqsWcGUjl","colab_type":"code","colab":{}},"source":["def accuracy_int_bit(value1,value2):\n","  accuracy = 0\n","  for i in range(3):\n","    if f'{value1:04b}'[i] == f'{value2:04b}'[i]:\n","      accuracy += 1\n","  return accuracy/4\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7f6re_0nGWMB","colab_type":"code","colab":{}},"source":["correct = 0\n","partial_correct =  0\n","for i in range(len(val_predicted)):\n","    if val_predicted[i] == val_labels[i]:\n","        correct +=1 \n","    partial_correct += accuracy_int_bit(val_predicted[i],val_labels[i])\n","print(\"accuracy of baseline model: {0}\".format(correct/len(val_predicted)))\n","print(\"Partial accuracy of baseline model: {0}\".format(partial_correct/len(val_predicted)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ovkV-hUjkmnx","colab_type":"text"},"source":["###Transfer Learning Architecture"]},{"cell_type":"code","metadata":{"id":"dRaKywVtk_Ja","colab_type":"code","colab":{}},"source":["class TransferModel(nn.Module): # This model will output an embedding state with shape [batch_num, 514]\n","  def __init__(self):\n","    super(TransferModel, self).__init__()\n","    self.name = \"transferNet\"\n","    self.conv1 = nn.Conv1d(2, 64, 11) # input channel is 2 for audio files\n","    self.conv2 = nn.Conv1d(64, 64, 9)\n","    self.conv3 = nn.Conv1d(64, 64, 7)\n","    self.conv4 = nn.Conv1d(64, 32, 5)\n","    self.conv5 = nn.Conv1d(32, 16, 5)\n","    self.conv6 = nn.Conv1d(16, 2, 5)\n","    self.pool = nn.MaxPool1d(2, 2) \n","    self.pool2 = nn.MaxPool1d(4, 4)\n","    self.bn1 = nn.BatchNorm1d(64)\n","    self.bn2 = nn.BatchNorm1d(64)\n","    self.bn3 = nn.BatchNorm1d(64)\n","    self.bn4 = nn.BatchNorm1d(32)\n","    self.bn5 = nn.BatchNorm1d(16)\n","    self.bn6 = nn.BatchNorm1d(2)\n","\n","  def forward(self, x):\n","    x = self.bn1(self.pool(F.relu(self.conv1(x)))) \n","    x = self.bn2(self.pool(F.relu(self.conv2(x)))) \n","    x = self.bn3(self.pool2(F.relu(self.conv3(x)))) \n","    x = self.bn4(self.pool2(F.relu(self.conv4(x))))\n","    x = self.bn5(self.pool2(F.relu(self.conv5(x))))\n","    x = self.bn6(self.pool2(F.relu(self.conv6(x))))\n","    x = x.view(-1, 254)\n","    return x\n","\n","def LoadFeatureModel(state_dict_path, transfered_model):\n","  # Load the best MyNet13 model\n","  MyNet13_best_state = torch.load(state_dict_path) #The state_dict file is stored in the shared google drive\n","  MyNet13_model = MyNet13()\n","  MyNet13_model.load_state_dict(MyNet13_best_state)\n","\n","  # Copy features from MyNet11 to Transfered_model\n","  transfered_model.conv1 = MyNet13_model.conv1\n","  transfered_model.conv2 = MyNet13_model.conv2\n","  transfered_model.conv3 = MyNet13_model.conv3\n","  transfered_model.conv4 = MyNet13_model.conv4\n","  transfered_model.conv5 = MyNet13_model.conv5\n","  transfered_model.conv6 = MyNet13_model.conv6\n","  transfered_model.bn1 = MyNet13_model.bn1\n","  transfered_model.bn2 = MyNet13_model.bn2\n","  transfered_model.bn3 = MyNet13_model.bn3\n","  transfered_model.bn4 = MyNet13_model.bn4\n","  transfered_model.bn5 = MyNet13_model.bn5\n","  transfered_model.bn6 = MyNet13_model.bn6\n","\n","  # Disable gradient for transfered_model\n","  for param in transfered_model.parameters():\n","      param.requires_grad = False\n","\n","  return transfered_model\n","\n","def LoadFeature(transfered_model, original_folder, batch_size=64): # Output the feature dataset\n","  train_loader, val_loader, test_loader = get_data_loaders(combined_audio_folder, batch_size=64)\n","  \n","  feature_train_loader = []\n","  feature_val_loader = []\n","  feature_test_loader = []\n","\n","  if use_cuda and torch.cuda.is_available():\n","    transfered_model = transfered_model.cuda()\n","\n","  for inputs, labels in train_loader:\n","    if use_cuda and torch.cuda.is_available():\n","      inputs = inputs.cuda()\n","      labels = labels.cuda()\n","    features = transfered_model(inputs)\n","    feature_train_loader.append([features, labels])\n","\n","  for inputs, labels in val_loader:\n","    if use_cuda and torch.cuda.is_available():\n","      inputs = inputs.cuda()\n","      labels = labels.cuda()\n","    features = transfered_model(inputs)\n","    feature_val_loader.append([features, labels]) \n","\n","  for inputs, labels in test_loader:\n","    if use_cuda and torch.cuda.is_available():\n","      inputs = inputs.cuda()\n","      labels = labels.cuda()\n","    features = transfered_model(inputs)\n","    feature_test_loader.append([features, labels]) \n","\n","  # Avoid pytorch to track weight update in feature data \n","  #features = torch.from_numpy(features.detach().numpy())\n","\n","  return feature_train_loader, feature_val_loader, feature_test_loader"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aviyPim4EA4T","colab_type":"code","colab":{}},"source":["class predictionNet(nn.Module):\n","    def __init__(self):\n","        super(predictionNet, self).__init__()\n","        self.name = \"prediction_net\"\n","        self.fc1 = nn.Linear(254, 100)\n","        self.fc2 = nn.Linear(100, 50)\n","        self.fc3 = nn.Linear(50, 4)\n","        self.dropout = nn.Dropout(0.2)\n","\n","    def forward(self, x):\n","        x = self.dropout(x)\n","        x = self.fc1(x)\n","        x = self.fc2(x)\n","        x = self.fc3(x)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JGnMmCdLEHNZ","colab_type":"code","colab":{}},"source":["def get_accuracy_transfer_learning(prediction_model, loader):\n","    \"\"\"\n","    Model output is considered correct only if all four outputs are correct.\n","    \"\"\"\n","    correct = 0\n","    total = 0\n","\n","    t = torch.Tensor([0])\n","\n","    for features, labels in loader:\n","        \n","        if use_cuda and torch.cuda.is_available():\n","          features = features.cuda()\n","          labels = labels.cuda()\n","          t = t.cuda()\n","        \n","        outputs = prediction_model(features)\n","        one_hot_outputs = (outputs >= t).int()\n","\n","        corr = sum(sum(one_hot_outputs == labels)).item()\n","\n","        correct += corr\n","        total += labels.shape[0] * 4\n","    \n","    return correct / total\n","\n","def transfer_train(prediction_model, train_loader=None, val_loader=None, batch_size=64, num_epochs=5, \n","            learning_rate=1e-4, checkpoint=False, checkpoint_name=None, checkpoint_bestonly=False,\n","            accuracy=get_accuracy_transfer_learning): \n","    torch.manual_seed(1)\n","    criterion = nn.MultiLabelSoftMarginLoss()\n","    optimizer = torch.optim.Adam(prediction_model.parameters(), lr=learning_rate)\n","    best_valacc = 0.0\n","\n","    if train_loader is not None and val_loader is not None:\n","        pass\n","    else:\n","        train_loader, val_loader, _ = get_data_loaders(combined_audio_folder, batch_size) \n","\n","    epoch_plot, losses, val_losses, train_acc, val_acc = [], [], [], [], []\n","\n","    for epoch in range(num_epochs):\n","        total_train_loss = 0\n","        start_time = time.time()\n","        prediction_model.train()\n","        for i, (features, labels) in enumerate(train_loader, 0):\n","\n","            if use_cuda and torch.cuda.is_available():\n","                features = features.cuda()\n","                labels = labels.cuda()\n","\n","            outputs = prediction_model(features)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            optimizer.zero_grad()\n","            \n","            total_train_loss += loss.item()\n","        total_train_loss = total_train_loss / (i+1) #/ 21502\n","        losses.append(total_train_loss)\n","        train_acc.append(accuracy(prediction_model,train_loader))\n","        \n","        # make validation predictions and calculate loss\n","        total_val_loss = 0\n","        num_val_batch = 0\n","        prediction_model.eval()\n","        with torch.no_grad():\n","          for features, labels in iter(val_loader):\n","\n","              if use_cuda and torch.cuda.is_available():\n","                  features = features.cuda()\n","                  labels = labels.cuda()\n","\n","              outputs = prediction_model(features)\n","              \n","              val_loss = criterion(outputs, labels)\n","\n","              total_val_loss += val_loss.item()\n","              num_val_batch += 1\n","        total_val_loss = total_val_loss / num_val_batch #/ 4608\n","        val_losses.append(float(total_val_loss))\n","        val_acc.append(accuracy(prediction_model, val_loader))\n","\n","        epoch_plot.append(epoch+1)\n","        end_time = time.time()\n","        elapsed_time = end_time - start_time\n","        print('Epoch:{}, Loss:{:.4f}, Val_Loss:{:.4f}, Train_acc:{:.4f}, Val_acc:{:.4f}, Total time elapsed: {:.2f} seconds'.format(\n","            epoch+1,\n","            float(total_train_loss),\n","            float(total_val_loss),\n","            float(train_acc[epoch]),\n","            float(val_acc[epoch]),\n","            elapsed_time))\n","\n","        # Save the current model (checkpoint) to a file\n","        if checkpoint:\n","            if (checkpoint_bestonly and val_acc[-1] > best_valacc):\n","                best_valacc = val_acc[-1]\n","                best_epoch = epoch+1\n","                best_model_state = prediction_model.state_dict()\n","            elif not checkpoint_bestonly:\n","                if checkpoint_name is not None:\n","                    model_path = \"/content/drive/My Drive/APS 360 Project/saved_models/{}_batch_size={}_lr={}_epoch={}\".format(checkpoint_name,batch_size,learning_rate,epoch)\n","                else:\n","                    model_path = \"/content/drive/My Drive/APS 360 Project/saved_models/batch_size={}_lr={}_epoch={}\".format(batch_size,learning_rate,epoch)\n","                torch.save(prediction_model.state_dict(), model_path)\n","\n","\n","    if checkpoint_name is not None:\n","        model_path = \"/content/drive/My Drive/APS 360 Project/saved_models/{}_batch_size={}_lr={}_epoch={}_best\".format(checkpoint_name,batch_size,learning_rate,best_epoch)\n","    else:\n","        model_path = \"/content/drive/My Drive/APS 360 Project/saved_models/batch_size={}_lr={}__epoch={}_best\".format(batch_size,learning_rate,best_epoch)\n","    torch.save(best_model_state, model_path)\n","\n","    # plotting\n","    plt.title(\"Training Curve\")\n","    plt.plot(epoch_plot, losses, label=\"Train\")\n","    plt.plot(epoch_plot, val_losses, label=\"Validation\")\n","    plt.xlabel(\"Iterations\")\n","    plt.ylabel(\"Loss\")\n","    plt.legend(loc='best')\n","    plt.show()\n","\n","    plt.title(\"Training Curve\")\n","    plt.plot(epoch_plot, train_acc, label=\"Train\")\n","    plt.plot(epoch_plot, val_acc, label=\"Validation\")\n","    plt.xlabel(\"Iterations\")\n","    plt.ylabel(\"Accuracy\")\n","    plt.legend(loc='best')\n","    plt.show()\n","\n","    print(\"Final Training Accuracy: {}\".format(train_acc[-1]))\n","    print(\"Final Validation Accuracy: {}\".format(val_acc[-1]))\n","    print (\"Maximum validation accuracy for this model is:\", max(val_acc),\n","           \"at epoch\", epoch_plot[val_acc.index(max(val_acc))],\"\\n\")\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N5nmLKt9qf4C","colab_type":"code","colab":{}},"source":["transfer_model = LoadFeatureModel('/content/drive/My Drive/APS 360 Project/saved_models/MyNet13_batch_size=16_lr=0.0003_best_0.7257valacc', TransferModel())\n","\n","# Get features vectors for training, validation and testing\n","feature_train_loader, feature_val_loader, feature_test_loader = LoadFeature(transfer_model, combined_audio_folder, batch_size=64)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YxvDj5ckELNP","colab_type":"code","colab":{}},"source":["torch.cuda.empty_cache()\n","prediction_model = predictionNet()\n","if use_cuda and torch.cuda.is_available():\n","  prediction_model = prediction_model.cuda()\n","transfer_train(prediction_model, train_loader=feature_train_loader, val_loader=feature_val_loader, learning_rate=0.0001, num_epochs=150, batch_size=64,\n","               checkpoint=True, checkpoint_name=\"Transfer_learning\", checkpoint_bestonly=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VdsRdbTfwzN9","colab_type":"code","colab":{}},"source":["# Get test accuracy for transfer learning model\n","best_model = predictionNet()\n","\n","state = torch.load(\"/content/drive/My Drive/APS 360 Project/saved_models/Transfer_learning_batch_size=64_lr=0.0001_epoch=8_best\")\n","best_model.load_state_dict(state)\n","\n","if use_cuda and torch.cuda.is_available():\n","  best_model = best_model.cuda()\n","\n","get_accuracy_2(transfer_model, best_model, feature_test_loader)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"reqU3MrylxZl","colab_type":"text"},"source":["###Convolutional Network Architecture"]},{"cell_type":"code","metadata":{"id":"2D2A5ig-MZIz","colab_type":"code","colab":{}},"source":["class multiNet(nn.Module):\n","    def __init__(self):\n","      super(multiNet, self).__init__()\n","      self.name = \"net\"\n","      self.conv1 = nn.Conv1d(2, 64, 11) # input channel is 2 for audio files\n","      self.conv2 = nn.Conv1d(64, 64, 9)\n","      self.conv3 = nn.Conv1d(64, 64, 7)\n","      self.conv4 = nn.Conv1d(64, 32, 5)\n","      self.conv5 = nn.Conv1d(32, 16, 5)\n","      self.conv6 = nn.Conv1d(16, 2, 5)\n","      self.fc1 = nn.Linear(254, 1)\n","      self.pool = nn.MaxPool1d(2, 2) \n","      self.pool2 = nn.MaxPool1d(4, 4)\n","      self.bn1 = nn.BatchNorm1d(64)\n","      self.bn2 = nn.BatchNorm1d(64)\n","      self.bn3 = nn.BatchNorm1d(64)\n","      self.bn4 = nn.BatchNorm1d(32)\n","      self.bn5 = nn.BatchNorm1d(16)\n","      self.bn6 = nn.BatchNorm1d(2)\n","      self.drop = nn.Dropout(0.3)\n","\n","    def forward(self, x):\n","        x = self.bn1(self.pool(F.relu(self.conv1(x)))) \n","        x = self.bn2(self.pool(F.relu(self.conv2(x)))) \n","        x = self.bn3(self.pool2(F.relu(self.conv3(x)))) \n","        x = self.bn4(self.pool2(F.relu(self.conv4(x))))\n","        x = self.bn5(self.pool2(F.relu(self.conv5(x))))\n","        x = self.bn6(self.pool2(F.relu(self.conv6(x))))\n","        # print(np.shape(x))\n","        x = self.drop(x)\n","        x0 = self.fc1(x)\n","        x1 = self.fc1(x)\n","        x2 = self.fc1(x)\n","        x3 = self.fc1(x)\n","        return x0,x1,x2,x3"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9kvrc89klpjt","colab_type":"text"},"source":["###Training Code"]},{"cell_type":"code","metadata":{"id":"gXvPF97NY_uI","colab_type":"code","colab":{}},"source":["class Stage2_test(nn.Module):\n","    \"\"\"\n","    Just a sample architecture I used to test the training code, but this is based on\n","    MyNet10 because our larger models gave me CUDA out of memory errors too frequently.\n","    \"\"\"\n","    def __init__(self):\n","        super(Stage2_test, self).__init__()\n","        self.name = \"net\"\n","        self.conv1 = nn.Conv1d(2, 16, 11) # input channel is 2 for audio files\n","        self.conv2 = nn.Conv1d(16, 32, 9)\n","        self.conv3 = nn.Conv1d(32, 16, 7)\n","        self.conv4 = nn.Conv1d(16, 8, 5)\n","        self.conv5 = nn.Conv1d(8, 2, 5)\n","        self.fc1 = nn.Linear(514, 1)\n","        self.pool = nn.MaxPool1d(2, 2) \n","        self.pool2 = nn.MaxPool1d(4, 4)\n","        self.bn1 = nn.BatchNorm1d(16)\n","        self.bn2 = nn.BatchNorm1d(32)\n","        self.bn3 = nn.BatchNorm1d(16)\n","        self.bn4 = nn.BatchNorm1d(8)\n","        self.bn5 = nn.BatchNorm1d(2)\n","        self.drop = nn.Dropout(0.3)\n","\n","    def forward(self, x):\n","        x = self.bn1(self.pool(F.relu(self.conv1(x)))) \n","        x = self.bn2(self.pool2(F.relu(self.conv2(x)))) \n","        x = self.bn3(self.pool2(F.relu(self.conv3(x)))) \n","        x = self.bn4(self.pool2(F.relu(self.conv4(x))))\n","        x = self.bn5(self.pool2(F.relu(self.conv5(x))))\n","        # print(np.shape(x))\n","        x = self.drop(x)\n","        x = x.view(-1, 514)\n","        x0 = self.fc1(x)\n","        x1 = self.fc1(x)\n","        x2 = self.fc1(x)\n","        x3 = self.fc1(x)\n","        return x0,x1,x2,x3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kvzWgn8qzkbi","colab_type":"code","colab":{}},"source":["def get_accuracy_multilabel(model, loader):\n","    \"\"\"\n","    Model output is considered correct only if all four outputs are correct.\n","    \"\"\"\n","    correct = 0\n","    total = 0\n","    for inputs, labels in loader:\n","        if use_cuda and torch.cuda.is_available():\n","           inputs = inputs.cuda()\n","           labels = labels.cuda()\n","        outputs = model(inputs)\n","        zeros = torch.from_numpy(np.zeros(np.shape(outputs))).cuda() if (use_cuda and torch.cuda.is_available()) else torch.from_numpy(np.zeros(np.shape(outputs)))\n","        batch_size = inputs.shape[0]\n","        corr = [True if all((outputs[i,:]>zeros[i,:]).long()==labels[i,:]) else False for i in range(batch_size)]\n","        # print(corr)\n","        correct += int(sum(corr))\n","        total += inputs.shape[0]\n","    return correct / total\n","\n","def get_part_accuracy_multilabel(model,loader):\n","    \"\"\"\n","    \"Part marks\" assigned for calculating model output correctness. Each correct\n","    binary classification is considered, even if other outputs corresponding to \n","    the same data sample are incorrect.\n","    \"\"\"\n","    correct = 0\n","    total = 0\n","    t = torch.Tensor([0])\n","    for inputs, labels in loader:   \n","        if use_cuda and torch.cuda.is_available():\n","          inputs = inputs.cuda()\n","          labels = labels.cuda()\n","          t = t.cuda()\n","        outputs = model(inputs)\n","        one_hot_outputs = (outputs >= t).int()\n","        corr = sum(sum(one_hot_outputs == labels)).item()\n","        correct += corr\n","        total += inputs.shape[0] * 4\n","    return correct / total\n","\n","\n","def train_multilabel(model, train_loader=None, valid_loader=None, batch_size=64, num_epochs=5, \n","            learning_rate=1e-4, checkpoint=False, checkpoint_name=None, checkpoint_bestonly=False): \n","    torch.manual_seed(1)\n","    criterion = nn.MultiLabelSoftMarginLoss()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","    best_valacc = 0.0\n","\n","    if train_loader is not None and valid_loader is not None:\n","        pass\n","    else:\n","        train_loader, val_loader, _ = get_data_loaders(combined_audio_folder, batch_size) \n","\n","    epoch_plot, losses, val_losses, train_acc, train_acc_part, val_acc, val_acc_part = [], [], [], [], [], [], []\n","    for epoch in range(num_epochs):\n","        total_train_loss = 0\n","        num_train_batch = 0\n","        for inputs, labels in iter(train_loader):\n","\n","            if use_cuda and torch.cuda.is_available():\n","                inputs = inputs.cuda()\n","                labels = labels.cuda()\n","            \n","            pred = model(inputs)\n","            loss = criterion(pred, labels)\n","            loss.backward()\n","            optimizer.step()\n","            optimizer.zero_grad()\n","            \n","            total_train_loss = total_train_loss + (float(loss.item()) /len(inputs))\n","            num_train_batch += 1\n","        total_train_loss = total_train_loss / num_train_batch #/ 21502\n","        losses.append(float(total_train_loss))\n","        train_acc.append(get_accuracy_multilabel(model,train_loader))\n","        train_acc_part.append(get_part_accuracy_multilabel(model,train_loader))\n","        \n","        # make validation predictions and calculate loss\n","        total_val_loss = 0\n","        num_val_batch = 0\n","        for inputs, labels in iter(val_loader):\n","            \n","            if use_cuda and torch.cuda.is_available():\n","                inputs = inputs.cuda()\n","                labels = labels.cuda()\n","\n","            pred = model(inputs)\n","            val_loss = criterion(pred,labels)\n","            \n","            total_val_loss = total_val_loss + (float(val_loss.item()) /len(inputs))\n","            num_val_batch += 1\n","        total_val_loss = total_val_loss / num_val_batch #/ 4608\n","        val_losses.append(float(total_val_loss))\n","        val_acc.append(get_accuracy_multilabel(model,val_loader))\n","        val_acc_part.append(get_part_accuracy_multilabel(model,val_loader))\n","\n","        epoch_plot.append(epoch)\n","        print('Epoch:{}, Loss:{:.4f}, Val_Loss:{:.4f}, Train_acc:{:.4f}, Train_acc_part:{:.4f}, Val_acc:{:.4f}, Val_acc_part:{:.4f}'.format(\n","            epoch+1,\n","            float(total_train_loss),\n","            float(total_val_loss),\n","            float(train_acc[epoch]),\n","            float(train_acc_part[epoch]),\n","            float(val_acc[epoch]),\n","            float(val_acc_part[epoch])))\n","\n","        # Save the current model (checkpoint) to a file\n","        if checkpoint:\n","            if (checkpoint_bestonly and val_acc_part[-1] > best_valacc):\n","                best_valacc = val_acc_part[-1]\n","                if checkpoint_name is not None:\n","                    model_path = \"/content/drive/My Drive/APS 360 Project/saved_models/{}_batch_size={}_lr={}_best\".format(checkpoint_name,batch_size,learning_rate,epoch)\n","                else:\n","                    model_path = \"/content/drive/My Drive/APS 360 Project/saved_models/batch_size={}_lr={}_best\".format(batch_size,learning_rate,epoch)\n","                torch.save(model.state_dict(), model_path)\n","            elif not checkpoint_bestonly:\n","                if checkpoint_name is not None:\n","                    model_path = \"/content/drive/My Drive/APS 360 Project/saved_models/{}_batch_size={}_lr={}_epoch={}\".format(checkpoint_name,batch_size,learning_rate,epoch)\n","                else:\n","                    model_path = \"/content/drive/My Drive/APS 360 Project/saved_models/batch_size={}_lr={}_epoch={}\".format(batch_size,learning_rate,epoch)\n","                torch.save(model.state_dict(), model_path)\n","\n","    # plotting\n","    plt.title(\"Training Curve\")\n","    plt.plot(epoch_plot, losses, label=\"Train\")\n","    plt.plot(epoch_plot, val_losses, label=\"Validation\")\n","    plt.xlabel(\"Iterations\")\n","    plt.ylabel(\"Loss\")\n","    plt.legend(loc='best')\n","    plt.show()\n","\n","    plt.title(\"Training Curve\")\n","    plt.plot(epoch_plot, train_acc, label=\"Train\")\n","    plt.plot(epoch_plot, val_acc, label=\"Validation\")\n","    plt.xlabel(\"Iterations\")\n","    plt.ylabel(\"Accuracy\")\n","    plt.legend(loc='best')\n","    plt.show()\n","\n","    plt.title(\"Training Curve\")\n","    plt.plot(epoch_plot, train_acc_part, label=\"Train\")\n","    plt.plot(epoch_plot, val_acc_part, label=\"Validation\")\n","    plt.xlabel(\"Iterations\")\n","    plt.ylabel(\"Part Accuracy\")\n","    plt.legend(loc='best')\n","    plt.show()\n","\n","    print(\"Final Training Accuracy: {}\".format(train_acc[-1]))\n","    print(\"Final Training (part) Accuracy: {}\".format(train_acc_part[-1]))\n","    print(\"Final Validation Accuracy: {}\".format(val_acc[-1]))\n","    print(\"Final Validation (part) Accuracy: {}\".format(val_acc_part[-1]))\n","    print (\"Maximum validation accuracy for this model is:\", max(val_acc),\n","           \"at epoch\", epoch_plot[val_acc.index(max(val_acc))],\"\\n\")\n","    print (\"Maximum validation (part) accuracy for this model is:\", max(val_acc_part),\n","           \"at epoch\", epoch_plot[val_acc_part.index(max(val_acc_part))],\"\\n\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aG4MB7Af24ph","colab_type":"code","colab":{}},"source":["### multi-output training using BCE loss for each output\n","# torch.cuda.empty_cache()\n","# model2 = Stage2_test()\n","# if use_cuda and torch.cuda.is_available():\n","#     model2 = model2.cuda()\n","# train_2(model2, learning_rate=0.0003, num_epochs=30, batch_size=32, checkpoint=True, checkpoint_name='Stage2_test', checkpoint_bestonly=True)\n","\n","### multi-label soft margin loss for single vector output\n","torch.cuda.empty_cache()\n","model2 = MyNet10()\n","if use_cuda and torch.cuda.is_available():\n","    model2 = model2.cuda()\n","train_multilabel(model2, learning_rate=0.0001, num_epochs=50, batch_size=32, checkpoint=True, checkpoint_name='Stage2_MyNet10', checkpoint_bestonly=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yxA5SVbaAVJZ","colab_type":"text"},"source":["# Overall Results"]},{"cell_type":"code","metadata":{"id":"wyEnHlDOAcW4","colab_type":"code","colab":{}},"source":["from sklearn.metrics import confusion_matrix\n","\n","train_loader1, val_loader1, test_loader1 = get_data_loaders(audioFolder, 16) \n","train_loader2, val_loader2, test_loader2 = get_data_loaders(combined_audio_folder, 16) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-5HkuczvAf9H","colab_type":"text"},"source":["### Stage 1 "]},{"cell_type":"code","metadata":{"id":"r0hn599kAZtw","colab_type":"code","colab":{}},"source":["# load stage 1 model\n","# load in best model and check training accuracy\n","stage_1_model = MyNet13()\n","saved_model = '/content/drive/My Drive/APS 360 Project/saved_models/MyNet13_batch_size=16_lr=0.0003_best_0.7257valacc'\n","stage_1_model.load_state_dict(torch.load(saved_model,map_location=torch.device('cpu')))\n","\n","# # do test set predictions for stage 1\n","print(\"Overall Test Accuracy (Stage 1):\",get_accuracy(stage_1_model.eval().cuda(),test_loader1))\n","print(\"Confusion Matrix:\")\n","stage_1_model = stage_1_model.eval().cuda()\n","all_outputs = []\n","all_labels = []\n","for inputs, labels in test_loader1:\n","    if use_cuda and torch.cuda.is_available():\n","        inputs = inputs.cuda()\n","        labels = labels.cuda()\n","    outputs = stage_1_model(inputs)\n","    pred = outputs.max(1, keepdim=True)[1].view_as(labels)\n","    all_outputs.extend(pred.tolist())\n","    all_labels.extend(labels.tolist())\n","print(confusion_matrix(all_labels,all_outputs))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EK-HSfzGAqNx","colab_type":"text"},"source":["### Stage 2 "]},{"cell_type":"code","metadata":{"id":"ZSGPxlitApXb","colab_type":"code","colab":{}},"source":["# load stage 2 model (non transfer learning)\n","stage_2_model = MyNet13()\n","saved_model = '/content/drive/My Drive/APS 360 Project/saved_models/Stage2_MyNet13_batch_size=16_lr=0.0003_best_0.6905valaccpart_0.2243valacc'\n","stage_2_model.load_state_dict(torch.load(saved_model,map_location=torch.device('cpu')))\n","\n","# do test set predictions for stage 2 (non transfer learning)\n","# print(get_part_accuracy_multilabel_class(stage_2_model.eval().cuda(),test_loader2,0))\n","print(\"Overall Test Accuracy (Stage 2)\",get_part_accuracy_multilabel(stage_2_model.eval().cuda(),test_loader2))\n","stage_2_model = stage_2_model.eval().cuda()\n","all_outputs = []\n","all_labels = []\n","t = torch.Tensor([0]).cuda()\n","for inputs, labels in test_loader2:\n","    if use_cuda and torch.cuda.is_available():\n","        inputs = inputs.cuda()\n","        labels = labels.cuda()\n","    outputs = stage_2_model(inputs)\n","    pred = (outputs >= t).int()\n","    all_outputs.extend(pred.tolist())\n","    all_labels.extend(labels.tolist())\n","\n","def multi_hot_to_num(label):\n","    # print(label)\n","    if label == [1,1,0,0]:\n","        return 0\n","    elif label == [1,0,1,0]:\n","        return 1\n","    elif label == [1,0,0,1]:\n","        return 2\n","    elif label == [0,1,1,0]:\n","        return 3\n","    elif label == [0,1,0,1]:\n","        return 4\n","    elif label == [0,0,1,1]:\n","        return 5\n","    else:\n","        # print(\"Error\")\n","        return 6\n","converted_labels = [multi_hot_to_num(label) for label in all_labels]\n","converted_outputs = [multi_hot_to_num(output) for output in all_outputs]\n","print(confusion_matrix(converted_labels,converted_outputs))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"czLAXcYFAzuv","colab_type":"text"},"source":["### Stage 2: Transfer Learning"]},{"cell_type":"code","metadata":{"id":"H7vIMchCAx4k","colab_type":"code","colab":{}},"source":["# load stage 2 transfer learning model\n","\n","# do test set predictions for stage 2 transfer learning"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_IrXI2d0uiP0","colab_type":"text"},"source":["**Demo on real data**"]},{"cell_type":"code","metadata":{"id":"vm5_bRHRutqh","colab_type":"code","colab":{}},"source":["regulated_length = 132299\n","\n","def slice_audio(waveform):\n","  audio_batch = []\n","  length = waveform.shape[1]\n","  for i in range(0, length, regulated_length):\n","    if i + regulated_length <= length:\n","      waveform_slices = waveform.narrow(1, i, regulated_length)\n","      audio_batch.append(waveform_slices)\n","  return audio_batch"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j_NtFnk3uys4","colab_type":"code","colab":{}},"source":["real_tru_vio, real_tru_vio_sample_rate = torchaudio.load('/content/real_tru_vio.mp3')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vhn0sCGJBhO_","colab_type":"code","colab":{}},"source":["real_tru, real_tru_sample_rate = torchaudio.load('/content/real_trumpet.mp3')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iU1eHfrevIcD","colab_type":"code","colab":{}},"source":["sliced_tru_vio = slice_audio(real_tru_vio)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WUHIbPEkB609","colab_type":"code","colab":{}},"source":["sliced_tru = slice_audio(real_tru)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VBKZp-U0ueKh","colab_type":"code","colab":{}},"source":["# Taking the best stage 2 model\n","num_classes = 4\n","\n","best_multi_class_model = MyNet13()\n","\n","state = torch.load(\"/content/drive/My Drive/APS 360 Project/saved_models/Stage2_MyNet13_batch_size=16_lr=0.0003_best_0.6905valaccpart_0.2243valacc\")\n","best_multi_class_model.load_state_dict(state)\n","\n","if use_cuda and torch.cuda.is_available():\n","  best_multi_class_model = best_multi_class_model.cuda()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UI6KwMCx3j9H","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596907645541,"user_tz":240,"elapsed":324,"user":{"displayName":"Tianxu An","photoUrl":"","userId":"04744487621159126983"}}},"source":["map = {0: 'acoustic guitar', 1: 'piano', 2: 'trumpet', 3: 'violin'}"],"execution_count":31,"outputs":[]},{"cell_type":"code","metadata":{"id":"Th12XNQzuXyL","colab_type":"code","colab":{}},"source":["def get_multi_class_real_data_accuracy(loader, label, best_model, transfer_model=None): \n","  best_model.eval()\n","  if transfer_model is not None:\n","    transfer_model.eval()\n","  predictions = torch.tensor([0, 0, 0, 0])\n","  correct = 0\n","  t = torch.Tensor([0])\n","  n = 0\n","  for input in loader: \n","    n += 1\n","    result = []     \n","    if use_cuda and torch.cuda.is_available():\n","      input = input.cuda()\n","      t = t.cuda()\n","      label = label.cuda()\n","      predictions = predictions.cuda()\n","    input = input.unsqueeze(0)\n","    if transfer_model is not None:\n","      input = transfer_model(input)\n","    output = best_model(input).squeeze()\n","    one_hot_output = (output >= t).int()\n","    for i in range(len(one_hot_output)):\n","      if one_hot_output[i] > 0:\n","        result.append(map[i])\n","    predictions = torch.add(predictions, one_hot_output)\n","    corr = sum(one_hot_output == label).item()\n","    correct += corr\n","    print(\"Prediction for sample {}: {} \\n\".format(n, result))\n","\n","  print(\"Accuracy: {} | Number of predictions for each class: {}\".format(correct / (len(loader) * 4), predictions))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0YRniEHV-GnP","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596908001341,"user_tz":240,"elapsed":333,"user":{"displayName":"Tianxu An","photoUrl":"","userId":"04744487621159126983"}}},"source":["def get_single_class_real_data_accuracy(loader, label, best_model):\n","  best_model.eval()\n","  predictions = torch.tensor([0, 0, 0, 0])\n","  correct = 0\n","  t = torch.Tensor([0])\n","  n = 0\n","  for input in loader:    \n","    n += 1\n","    result = []  \n","    if use_cuda and torch.cuda.is_available():\n","      input = input.cuda()\n","    input = input.unsqueeze(0)\n","    output = best_model(input).squeeze()\n","    pred_index = output.max(0, keepdim=True)[1].item()\n","    result.append(map[pred_index])\n","    predictions[pred_index] += 1\n","    correct += int(pred_index == label)\n","    print(\"Prediction for sample {}: {} \\n\".format(n, result))\n","  print(\"Accuracy: {}% | Number of predictions for each class: {}\".format((correct / len(loader) * 100), predictions))"],"execution_count":43,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tl-uYCCKvR2L","colab_type":"code","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1596473523903,"user_tz":240,"elapsed":886,"user":{"displayName":"Ahmed Nissal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9VwLQFEJ_MtOfpfXUsU01xgifV4R3Dn381ewo6g=s64","userId":"01795038140591039031"}},"outputId":"dc99aca2-4f7a-4433-bced-7b499af9863d"},"source":["get_multi_class_real_data_accuracy(sliced_tru_vio, torch.tensor([0, 0, 1, 1]), best_multi_class_model)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Prediction for sample 1: ['acoustic guitar', 'piano', 'trumpet'] \n","\n","Prediction for sample 2: ['piano', 'trumpet'] \n","\n","Prediction for sample 3: ['piano', 'trumpet'] \n","\n","Prediction for sample 4: ['acoustic guitar', 'violin'] \n","\n","Prediction for sample 5: ['trumpet', 'violin'] \n","\n","Prediction for sample 6: ['trumpet', 'violin'] \n","\n","Prediction for sample 7: ['acoustic guitar', 'piano'] \n","\n","Prediction for sample 8: ['piano', 'trumpet'] \n","\n","Prediction for sample 9: ['violin'] \n","\n","Prediction for sample 10: ['piano', 'violin'] \n","\n","Prediction for sample 11: ['piano', 'trumpet', 'violin'] \n","\n","Prediction for sample 12: ['piano', 'trumpet', 'violin'] \n","\n","Prediction for sample 13: ['trumpet', 'violin'] \n","\n","Prediction for sample 14: ['piano', 'trumpet', 'violin'] \n","\n","Prediction for sample 15: ['piano', 'trumpet'] \n","\n","Prediction for sample 16: ['trumpet', 'violin'] \n","\n","Prediction for sample 17: ['acoustic guitar', 'piano', 'trumpet'] \n","\n","Prediction for sample 18: ['piano', 'violin'] \n","\n","Prediction for sample 19: ['piano', 'violin'] \n","\n","Prediction for sample 20: ['piano', 'trumpet'] \n","\n","Prediction for sample 21: ['trumpet', 'violin'] \n","\n","Prediction for sample 22: ['piano', 'trumpet', 'violin'] \n","\n","Prediction for sample 23: ['acoustic guitar', 'piano', 'trumpet'] \n","\n","Accuracy: 0.6086956521739131 | Number of predictions for each class: tensor([ 5, 16, 17, 14], device='cuda:0')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"K4jbdT7tDuOs","colab_type":"code","colab":{}},"source":["num_classes = 4\n","best_single_class_model = MyNet13()\n","state = torch.load(\"/content/drive/My Drive/APS 360 Project/saved_models/MyNet13_batch_size=16_lr=0.0003_best_0.7257valacc\")\n","best_single_class_model.load_state_dict(state)\n","\n","if use_cuda and torch.cuda.is_available():\n","  best_single_class_model = best_single_class_model.cuda()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J8DmH4E0DUV_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":923},"executionInfo":{"status":"ok","timestamp":1596908007979,"user_tz":240,"elapsed":395,"user":{"displayName":"Tianxu An","photoUrl":"","userId":"04744487621159126983"}},"outputId":"62bf6d56-1132-43c8-ae0a-f6072c67e2b2"},"source":["get_single_class_real_data_accuracy(sliced_tru[:25], 2, best_single_class_model)"],"execution_count":44,"outputs":[{"output_type":"stream","text":["Prediction for sample 1: ['trumpet'] \n","\n","Prediction for sample 2: ['trumpet'] \n","\n","Prediction for sample 3: ['trumpet'] \n","\n","Prediction for sample 4: ['trumpet'] \n","\n","Prediction for sample 5: ['trumpet'] \n","\n","Prediction for sample 6: ['trumpet'] \n","\n","Prediction for sample 7: ['trumpet'] \n","\n","Prediction for sample 8: ['trumpet'] \n","\n","Prediction for sample 9: ['trumpet'] \n","\n","Prediction for sample 10: ['trumpet'] \n","\n","Prediction for sample 11: ['trumpet'] \n","\n","Prediction for sample 12: ['trumpet'] \n","\n","Prediction for sample 13: ['trumpet'] \n","\n","Prediction for sample 14: ['trumpet'] \n","\n","Prediction for sample 15: ['trumpet'] \n","\n","Prediction for sample 16: ['trumpet'] \n","\n","Prediction for sample 17: ['trumpet'] \n","\n","Prediction for sample 18: ['trumpet'] \n","\n","Prediction for sample 19: ['trumpet'] \n","\n","Prediction for sample 20: ['trumpet'] \n","\n","Prediction for sample 21: ['trumpet'] \n","\n","Prediction for sample 22: ['trumpet'] \n","\n","Prediction for sample 23: ['trumpet'] \n","\n","Prediction for sample 24: ['trumpet'] \n","\n","Prediction for sample 25: ['trumpet'] \n","\n","Accuracy: 100.0% | Number of predictions for each class: tensor([ 0,  0, 25,  0])\n"],"name":"stdout"}]}]}